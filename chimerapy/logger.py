# Built-in Imports
from typing import Dict, Any
import multiprocessing as mp
import queue
import threading
import collections
import json
import pathlib
import time
import os
import queue
import signal

# Third-party imports

# ChimeraPy Library
from .core.tools import PortableQueue, threaded
from .core.video import VideoEntry
from .core.tabular import TabularEntry, ImageEntry
from .core.three_d import PointCloudEntry
from .base_actor import BaseActor

# Resource:
# https://stackoverflow.com/questions/8489684/python-subclassing-multiprocessing-process

class Logger(BaseActor):
    """Subprocess tasked with logging in an annotated and organized manner.

    The ``Logger`` focuses on logging data chunks, while keeping record
    of meta data for easy reconstruction of data processing throughout 
    the pipeline. The ``Logger`` is primarly used in the forward 
    propagation of the data through the pipeline.

    """

    def __init__(
            self,
            logdir:pathlib.Path,
            experiment_name:str,
            logging_queue:PortableQueue,
            message_to_queue:PortableQueue,
            message_from_queue:PortableQueue,
            verbose:bool=False
        ):
        """Construct a ``Logger`` to log data in a structured manner.

        Args:
            logdir (pathlib.Path): The log directory to save the folder \
                generated by the ``Logger`` instance.

            experiment_name (str): The name of the experiment, which \
                will be used to create a unique folder for the ``Logger``.

            logging_queue (PortableQueue): The logging queue where the \
                ``Logger`` will ``get`` to process and save into memory.

            message_to_queue (PortableQueue): The messaging queue used to \
                send messages to the ``Logger``.

            message_from_queue (PortableQueue): The messaging queue used to \
                receive messages from the ``Logger``.

        """
        super().__init__(
            message_to_queue=message_to_queue,
            message_from_queue=message_from_queue
        )
        
        # Save the input parameters
        self.logdir = logdir
        self.experiment_name = experiment_name
        self.experiment_dir = self.logdir / self.experiment_name
        self.logging_queue = logging_queue
        self.verbose = verbose

        # Keeping records of all logged data
        self.records = collections.defaultdict(dict)
        self.threads = collections.defaultdict(dict)
        self.queues = collections.defaultdict(dict)
        self.meta_data = {}
        self.dtype_to_class = {
            'tabular': TabularEntry,
            'image': ImageEntry,
            'video': VideoEntry,
            'point_cloud': PointCloudEntry
        }
        
        # Create a lock to prevent multiple threads from executing 
        # interferring code at the same time
        self.lock = threading.Lock()
        
        # Create the folder if it doesn't exist 
        if not self.logdir.exists():
            os.mkdir(self.logdir)

        # Create the experiment dir
        if not self.experiment_dir.exists():
            os.mkdir(self.experiment_dir)
        
        # Create a JSON file with the session's meta
        self.meta_data = {
            'id': self.experiment_name, 
            'subsessions': [], 
            'records': collections.defaultdict(dict)
        }
        self._save_meta_data()

        # Adding specific function class from the message
        self.subclass_message_to_functions.update({
        })

    def message_logging_status(self, data_chunk):
        """message_from function to report the number of logged data."""

        # Create the message
        logging_status_message = {
            'header': 'UPDATE',
            'body': {
                'type': 'COUNTER',
                'content': {
                    'uuid': data_chunk['uuid'],
                    'num_of_logged_data': self.num_of_logged_data,
                }
            }
        }

        # Send the message
        try:
            self.message_from_queue.put(logging_status_message.copy(), timeout=0.5)
        except queue.Full:
            print("Logger: Logging_status_message failed to send!")

    def message_logger_finished(self):
        """message_from function to report logging completion."""

        # Create the message
        logging_status_message = {
            'header': 'META',
            'body': {
                'type': 'END',
                'content': {}
            }
        }

        # Send the message
        try:
            self.message_from_queue.put(logging_status_message.copy(), timeout=0.5)
        except queue.Full:
            print("Logger: Logging_status_message failed to send!")
    
    def _save_meta_data(self):
        """Save the meta to a JSON file."""
        # Added lock to prevent interfering in writing data in shared file
        self.lock.acquire()
        with open(self.experiment_dir / 'meta.json', "w") as json_file:
            json.dump(self.meta_data, json_file)
        self.lock.release()

    def create_entry(self, data_chunk:Dict[str, Any]):
        """Create an entry and add it to the records and meta data.

        Args:
            data_chunk (Dict[str, Any]): The data chunk that details the \
            data type and name for the entry.

        """
        # New session processing (get the entry's directory)
        if data_chunk['session_name'] == 'root':
            entry_dir = self.experiment_dir

        else:# Add the new session to the subsesssions list
            if data_chunk['session_name'] not in self.meta_data['subsessions']:
                self.meta_data['subsessions'].append(data_chunk['session_name'])
            entry_dir = self.experiment_dir / data_chunk['session_name']

        # Selecting the class
        entry_cls = self.dtype_to_class[data_chunk['dtype']]

        # Creating the entry and recording in meta data
        self.records[data_chunk['session_name']][data_chunk['name']] = entry_cls(entry_dir, data_chunk['name'])
        entry_meta_data = {
            'dtype': data_chunk['dtype'],
            'start_time': str(data_chunk['data'].iloc[0]._time_),
            'end_time': str(data_chunk['data'].iloc[-1]._time_),
        }
        self.meta_data['records'][data_chunk['session_name']][data_chunk['name']] = entry_meta_data
        self._save_meta_data()
  
    @threaded
    def entry_thread(self, queue:queue.Queue):
        """Create a new thread for an entry of data.

        Each thread has the individual responsibility of update a specific 
        entry. The input queue is the matching data flow for said entry.

        Args:
            queue (queue.Queue): The queue that feeds data to the thread.

        """
        # Continue processing
        while True:

            # If we have data to log, work on it
            if queue.qsize() != 0:
                
                # Log the data
                data_chunk = queue.get()
                self.flush(data_chunk)

                # Notify that the data is complete!
                self.num_of_logged_data += 1
                self.message_logging_status(data_chunk)

            # Else, sleep to save resources
            else:
                time.sleep(0.1)

            # Breaking condition
            if self.thread_exit.is_set() and queue.qsize() == 0:
                break
   
    def flush(self, data:Dict[str, Any]):
        """Flush out unsaved logged changes by saving and clearing cache.

        This function saves the logged changes, where each logged 
        information is attached to an ``Entry`` and its name. All the 
        logged data with the same ``entry_name`` will be stored together.
        If the logged data is the first time saved, there are additional
        preparation required (such as opening files, creating directories,
        and creating ``Entry`` instances). Else, the logged data is 
        appended to the ``Entry``.

        Args:
            data (Dict[str, Any]): The data chuck containing ``uuid``, \
                ``session_name``, ``name``, ``data``, and ``dtype`` keys. \
                This information fully describes the content inside the \
                ``data``.

        """
        # Test that the new data entry is valid to the type of entry
        assert isinstance(self.records[data['session_name']][data['name']], self.dtype_to_class[data['dtype']]), \
            f"Entry Type={self.records[data['session_name']][data['name']]} should match input data dtype {data['dtype']}"

        # Need to update the end_time for meta_data
        if len(data['data']) > 0:
            end_time_stamp = str(data['data'].iloc[-1]._time_)
            self.meta_data['records'][data['session_name']][data['name']]['end_time'] = end_time_stamp 
            self._save_meta_data()

        # Debugging
        if self.verbose:
            print(f"Logger: saving {data['session_name']} - {data['name']}")

        # Now that we have account for both scenarios, just log data!
        self.records[data['session_name']][data['name']].append(data)
        self.records[data['session_name']][data['name']].flush()

    def run(self):
        """Run the ``Logger``.

        This is the main routine of the ``Logger`` which frequently 
        checks for new logged data and saves it to an ``Entry``. More of
        the details for saving the logged data can be found in the 
        ``flush`` method.

        """
        # Perform process setup
        self.setup()
        
        # Ignore SIGINT signal
        signal.signal(signal.SIGINT, signal.SIG_IGN)

        # Keeping track of processed data
        self.num_of_logged_data = 0

        # Continuously check if there are data to log and save
        while True: 

            # First check if there is an item in the queue
            if self.logging_queue.qsize() != 0:

                # Get the data frome the queue and calculate the memory usage
                data_chunk = self.logging_queue.get(block=True)

                # Extract the session name and entry name
                session_name = data_chunk['session_name']
                entry_name = data_chunk['name']
                
                # Debugging
                if self.verbose:
                    print(f"Loader: ``get`` data chunk with {session_name} - {entry_name}")

                # Determine if the data chunk is for a new entry, if so,
                # then create a new thread and pass that data!
                if session_name not in self.records.keys() or entry_name not in self.records[session_name].keys():
                   
                    # Create the entry for the data_chunk
                    self.create_entry(data_chunk)

                    # Setup the thread with its queue
                    new_entry_queue = queue.Queue() 
                    new_entry_thread = self.entry_thread(queue=new_entry_queue)

                    # Start the thread
                    new_entry_thread.start()

                    # Store them
                    self.queues[session_name][entry_name] = new_entry_queue
                    self.threads[session_name][entry_name] = new_entry_thread

                # Now that we have ensure that a thread exists, feed the
                # data chunk to that thread
                self.queues[session_name][entry_name].put(data_chunk)

            else:
                time.sleep(0.1)

            # Break Condition
            if self.thread_exit.is_set() and self.logging_queue.qsize() == 0:
                break

        # Wait until all the logging threads have stopped!
        for session_name in self.threads.keys():
            for entry_name in self.threads[session_name].keys():
                thread = self.threads[session_name][entry_name]
                thread.join()

        # Sending message that the Logger finished!
        self.message_logger_finished()

        # Save all the entries and close!
        self.shutdown()
        self.close()

    def shutdown(self):
        """Shutdown the ``Logger```.

        This routine is important to properly close the ``Entry`` 
        instances created by the ``Logger``. The ``Entry`` is required
        to ``close`` to fully save the file.

        """
        # Then close all the entries
        for session in self.records.values():
            for entry in session.values():
                entry.close()
